import os
from torch.utils.data import Dataset, DataLoader
import numpy as np
import torch
from ResNet_3D import ResNet18_3D_7stream_LSTM
from Nii_utils import NiiDataRead
import argparse
import pandas as pd
import shutil


class Dataset_for_external_validation(Dataset):
    """
    Dataset class for external validation
    Loads 7-modal 3D medical images from NIfTI files for validation tasks
    """
    def __init__(self, data_dir, split_path):
        self.data_dir = data_dir

        # Read case ID list from split file
        with open(split_path, 'r') as f:
            ID_list_original = f.readlines()
        self.ID_list = [n.strip('\n') for n in ID_list_original]
        
        self.len = len(self.ID_list)

    def __getitem__(self, idx):
        # Get case ID by index
        ID = self.ID_list[idx]

        # Read 7 modal images from NIfTI files
        img_1, _, _, _ = NiiDataRead(os.path.join(self.data_dir, ID, '1_img.nii.gz'))
        img_2, _, _, _ = NiiDataRead(os.path.join(self.data_dir, ID, '2_img.nii.gz'))
        img_3, _, _, _ = NiiDataRead(os.path.join(self.data_dir, ID, '3_img.nii.gz'))
        img_4, _, _, _ = NiiDataRead(os.path.join(self.data_dir, ID, '4_img.nii.gz'))
        img_5, _, _, _ = NiiDataRead(os.path.join(self.data_dir, ID, '5_img.nii.gz'))
        img_6, _, _, _ = NiiDataRead(os.path.join(self.data_dir, ID, '6_img.nii.gz'))
        img_7, _, _, _ = NiiDataRead(os.path.join(self.data_dir, ID, '7_img.nii.gz'))

        # Concatenate 7 modalities along the last axis (channel dimension)
        img = np.concatenate((img_1[..., np.newaxis],
                              img_2[..., np.newaxis],
                              img_3[..., np.newaxis],
                              img_4[..., np.newaxis],
                              img_5[..., np.newaxis],
                              img_6[..., np.newaxis],
                              img_7[..., np.newaxis]), axis=-1)

        # Convert to PyTorch tensor and reorder dimensions (C, D, H, W)
        img = torch.from_numpy(img).permute(3, 0, 1, 2)
        
        # Add channel dimension to each modal image (shape: [1, D, H, W])
        img_1 = img[0].unsqueeze(0)
        img_2 = img[1].unsqueeze(0)
        img_3 = img[2].unsqueeze(0)
        img_4 = img[3].unsqueeze(0)
        img_5 = img[4].unsqueeze(0)
        img_6 = img[5].unsqueeze(0)
        img_7 = img[6].unsqueeze(0)
        
        return img_1, img_2, img_3, img_4, img_5, img_6, img_7, ID

    def __len__(self):
        # Return total number of validation cases
        return self.len

# Parse command line arguments
parser = argparse.ArgumentParser(description='External validation for S1/S4 medical image classification tasks')
# Make model_path optional (auto-generated if not specified)
parser.add_argument('--model_path', type=str, default=None, 
                    help='Path to pre-trained model checkpoint (auto-generated by --task if not specified)')
parser.add_argument('--output_csv', type=str, default=None, 
                    help='Path to save output CSV file (auto-generated if not specified)')
parser.add_argument('--save_dir', type=str, default=None, 
                    help='Directory to save validation results (auto-generated if not specified)')
# Add task parameter (restrict to S1/S4 to avoid invalid inputs)
parser.add_argument('--task', type=str, default='S1', choices=['S1', 'S4'], 
                    help='Task identifier (S1 or S4) - controls model path, output paths and column naming')
# Optional: Allow overriding model path components (batch size/epoch/seed)
parser.add_argument('--bs', type=int, default=4, help='Batch size used in model training (for auto model path)')
parser.add_argument('--epoch', type=int, default=200, help='Epochs used in model training (for auto model path)')
parser.add_argument('--seed', type=int, default=42, help='Random seed used in model training (for auto model path)')
args = parser.parse_args()


# -------------------------- Task-specific configurations --------------------------
# Base directory for preprocessed validation data
data_dir = "./data/preprocessed_val/"
# Path to validation case split file
val_split_path = './relevant_files/val.txt'
# Number of classification classes (binary classification)
num_class = 2

# Auto-generate model path if not specified (task-specific)
if args.model_path is None:
    # Dynamic model path: replace S1/S4 in the path with --task
    args.model_path = f'./trained_models/trained_models_{args.task}/bs{args.bs}_epoch{args.epoch}_seed{args.seed}/best_ACC_val.pth'

# Auto-generate save directory if not specified (task-specific)
if args.save_dir is None:
    args.save_dir = f'./results/val_results/val_results_{args.task}'
save_dir = args.save_dir

# Create/reset save directory
if os.path.exists(save_dir):
    shutil.rmtree(save_dir)
os.makedirs(save_dir, exist_ok=True)

print(f'External validation results will be saved to: {save_dir}')
print(f'Using model path (auto-generated for {args.task}): {args.model_path}')

# -------------------------- Dataset loading --------------------------
print('Loading external validation dataset...')
# Initialize validation dataset (using task-agnostic loader)
exval_data = Dataset_for_external_validation(data_dir, val_split_path)
# Create dataloader (batch size=1 for individual case processing)
exval_dataloader = DataLoader(dataset=exval_data, batch_size=1, shuffle=False, drop_last=False)

print(f'External validation dataset length: {exval_data.len}')

# -------------------------- Model initialization --------------------------
print('Loading pre-trained model...')
# Initialize 3D ResNet model with 7-stream LSTM (binary classification)
net = ResNet18_3D_7stream_LSTM(in_channels=1, n_classes=num_class, pretrained=False, no_cuda=False)

# Use multi-GPU training if available
if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs for inference")
net = torch.nn.DataParallel(net)
net = net.cuda()  # Move model to GPU

# Validate model path exists
if not os.path.exists(args.model_path):
    raise FileNotFoundError(
        f"Model checkpoint not found at: {args.model_path}\n"
        f"Check if the task ({args.task}) or model training parameters (bs={args.bs}, epoch={args.epoch}, seed={args.seed}) are correct."
    )

# Load pre-trained weights
net.load_state_dict(torch.load(args.model_path))
print(f'Successfully loaded model from: {args.model_path}')

# -------------------------- Inference --------------------------
print('Starting external validation prediction...')

net.eval()  # Set model to evaluation mode
predictions = []  # Store prediction results for CSV output

# Task-specific column name for prediction probabilities
subtask_num = '1' if args.task == 'S1' else '4'
prob_column = f'Subtask{subtask_num}_prob_{args.task}'

# Disable gradient computation for inference (speed up & reduce memory usage)
with torch.no_grad():
    for i, (T1W1_imgs, T2W2_imgs, DWI_imgs, GED1_imgs, GED2_imgs, GED3_imgs, GED4_imgs, case_ids) in enumerate(exval_dataloader):
        # Move input tensors to GPU and convert to float32
        T1W1_imgs = T1W1_imgs.cuda().float()
        T2W2_imgs = T2W2_imgs.cuda().float()
        DWI_imgs = DWI_imgs.cuda().float()
        GED1_imgs = GED1_imgs.cuda().float()
        GED2_imgs = GED2_imgs.cuda().float()
        GED3_imgs = GED3_imgs.cuda().float()
        GED4_imgs = GED4_imgs.cuda().float()

        # Forward pass through the model
        outputs = net(T1W1_imgs, T2W2_imgs, DWI_imgs, GED1_imgs, GED2_imgs, GED3_imgs, GED4_imgs)

        # Compute softmax to get class probabilities
        outputs = torch.softmax(outputs, dim=1)
        # Extract probability for positive class (class 1)
        prob_class_1 = outputs[:, 1].cpu().numpy()

        # Store predictions for each case in the batch
        for j, case_id in enumerate(case_ids):
            predictions.append({
                'Case': case_id,
                'Setting': 'Contrast',  # Modify this if S4 requires a different setting
                prob_column: prob_class_1[j]
            })
        
        # Print progress
        print(f'Processed {i+1}/{len(exval_dataloader)} cases')

# -------------------------- Result saving & analysis --------------------------
# Convert predictions to DataFrame
df = pd.DataFrame(predictions)

# Auto-generate output CSV path if not specified (task-specific)
if args.output_csv is None:
    args.output_csv = f'val_predictions_{args.task}.csv'
output_path = os.path.join(save_dir, args.output_csv)
df.to_csv(output_path, index=False)

# Print summary statistics
print('\n' + '='*50)
print('EXTERNAL VALIDATION COMPLETED')
print('='*50)
print(f'Total cases processed: {len(predictions)}')
print(f'Results saved to: {output_path}')
print('='*50)

print('\nFirst 10 predictions:')
print(df.head(10).to_string(index=False))

print(f'\nPrediction probability statistics ({prob_column}):')
# Statistical analysis for task-specific probability column
print(f'Mean: {df[prob_column].mean():.4f}')
print(f'Standard Deviation: {df[prob_column].std():.4f}')
print(f'Minimum: {df[prob_column].min():.4f}')
print(f'Maximum: {df[prob_column].max():.4f}')

# Calculate predicted classes (threshold = 0.5 for binary classification)
predicted_classes = (df[prob_column] >= 0.5).astype(int)
class_counts = predicted_classes.value_counts().sort_index()
print(f'\nPredicted class distribution (threshold=0.5):')
print(f'Class 0 (Negative): {class_counts.get(0, 0)} cases')
print(f'Class 1 (Positive): {class_counts.get(1, 0)} cases')

print(f'\nExternal validation for {args.task} completed successfully!')
print(f'CSV file saved at: {output_path}')